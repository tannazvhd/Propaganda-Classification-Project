{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    import random\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string\n",
    "    from string import digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import classification_report\n",
    "    import transformers\n",
    "    from transformers import AutoModel, BertTokenizerFast\n",
    "    from ipywidgets import IntProgress\n",
    "    from tqdm import tqdm\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from spacy.lang.en import English\n",
    "    import re\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train dataset into train, validation and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train = pd.read_excel('Data/trainDataset.xlsx', engine='openpyxl')\n",
    "test = pd.read_excel('Data/testDataset.xlsx', engine='openpyxl')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Training data shape: ', train.shape)\n",
    "print('Testing data shape: ', test.shape)\n",
    "\n",
    "# First few rows of the training dataset\n",
    "train.head()\n",
    "\n",
    "# First few rows of the testing dataset\n",
    "test.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Missing Values treatment in the dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Missing values in training set\n",
    "train.isnull().sum()\n",
    "#Missing values in test set\n",
    "test.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysis of the SUBJprop Column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Propaganda Sentence\n",
    "print(\"Propaganda Sentence example :\",train[train['SUBJprop']==1]['Sentence'].values[0])\n",
    "#Non-Propaganda Sentence\n",
    "print(\"Non-Propaganda Sentence example :\",train[train['SUBJprop']==0]['Sentence'].values[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution of the SUBJprop Column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train['SUBJprop'].value_counts()\n",
    "# train['SUBJprop'].value_counts(normalize=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot Distribution of SUBJprop Column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train['SUBJprop'].value_counts(normalize=True).iplot(kind='bar',\n",
    "    yTitle='Percentage', \n",
    "    linecolor='black', \n",
    "    opacity=0.7,\n",
    "    color='red',\n",
    "    theme='pearl',\n",
    "    bargap=0.6,\n",
    "    gridcolor='white',\n",
    "    title='Distribution of SUBJprop Column in the training set')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test['SUBJprop'].value_counts(normalize=True).iplot(kind='bar',\n",
    "    yTitle='Percentage', \n",
    "    linecolor='black', \n",
    "    opacity=0.7,\n",
    "    color='red',\n",
    "    theme='pearl',\n",
    "    bargap=0.6,\n",
    "    gridcolor='white',\n",
    "    title='Distribution  of SUBJprop column in the test set')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    set(stopwords.words('english'))\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Applying the cleaning function to both test and training datasets\n",
    "train['text_clean'] = train['Sentence'].apply(str).apply(lambda x: text_preprocessing(x))\n",
    "test['text_clean'] = test['Sentence'].apply(str).apply(lambda x: text_preprocessing(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyzing Text Statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train['text_len'] = train['text_clean'].astype(str).apply(len)\n",
    "train['text_word_count'] = train['text_clean'].apply(lambda x: len(str(x).split()))\n",
    "train.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos = train[train['SUBJprop']==1]\n",
    "neg = train[train['SUBJprop']==0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence length analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos['text_len'].iplot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    xTitle='text length',\n",
    "    linecolor='black',\n",
    "    color='red',\n",
    "    yTitle='count',\n",
    "    title='Positive Text Length Distribution')\n",
    "\n",
    "neg['text_len'].iplot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    xTitle='text length',\n",
    "    linecolor='black',\n",
    "    color='green',\n",
    "    yTitle='count',\n",
    "    title='Negative Text Length Distribution')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text word count analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos['text_word_count'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='text length',\n",
    "    linecolor='black',\n",
    "    color='red',\n",
    "    yTitle='count',\n",
    "    title='Positive Text word count')\n",
    "\n",
    "neg['text_word_count'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='text length',\n",
    "    linecolor='black',\n",
    "    color='green',\n",
    "    yTitle='count',\n",
    "    title='Negative Text word count')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distribution of top unigrams"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_unigrams = get_top_n_words(pos['text_clean'],20)\n",
    "neg_unigrams = get_top_n_words(neg['text_clean'],20)\n",
    "\n",
    "\n",
    "#for word, freq in top_unigrams:\n",
    "    #print(word, freq)\n",
    "df1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\n",
    "df1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 1-Word in Propaganda Sentences',orientation='h')\n",
    "\n",
    "df2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\n",
    "df2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 1-Word in Non-Propaganda Sentences',orientation='h')\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Distribution of top Bigrams"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\n",
    "neg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\n",
    "\n",
    "#for word, freq in top_bigrams:\n",
    "    #print(word, freq)\n",
    "df1 = pd.DataFrame(pos_bigrams, columns = ['Text' , 'count'])\n",
    "df1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 2-Word in Propaganda Sentences',orientation='h')\n",
    "\n",
    "df2 = pd.DataFrame(neg_bigrams, columns = ['Text' , 'count'])\n",
    "df2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 2-Word in Non-Propaganda Sentences',orientation='h')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Distribution of top Trigrams "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\n",
    "neg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\n",
    "\n",
    "df1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\n",
    "df1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Trigrams in positve text',orientation='h')\n",
    "\n",
    "df2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\n",
    "df2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n",
    "    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Trigrams in negative text',orientation='h')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}