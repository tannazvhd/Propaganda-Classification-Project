{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import All Packages\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "    import random\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string\n",
    "    from string import digits\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import classification_report\n",
    "    import transformers\n",
    "    from transformers import AutoModel, BertTokenizerFast\n",
    "    from ipywidgets import IntProgress\n",
    "    from tqdm import tqdm\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import BERT Model, BERT Tokenizer and Torch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# import Torch\n",
    "device = torch.device(\"cpu\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predefined CLass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Propaganda:\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1\n",
    "\n",
    "class Review:\n",
    "    def __init__(self,sentence,SUBJprop):\n",
    "        self.sentence = sentence\n",
    "        self.SUBJprop = SUBJprop\n",
    "        self.propaganda = SUBJprop\n",
    "\n",
    "\n",
    "class ReviewContainer:\n",
    "    def __init__(self,reviews):\n",
    "        self.reviews = reviews\n",
    "\n",
    "    def get_sentence(self):\n",
    "        return [x.sentence for x  in self.reviews]\n",
    "\n",
    "    def get_propaganda(self):\n",
    "        return [int(x.propaganda) for x in self.reviews]\n",
    "\n",
    "    def evenly_distribute(self):\n",
    "        negative = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), self.reviews))\n",
    "        positive = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), self.reviews))\n",
    "        negative_shrunk = negative[:len(positive)]\n",
    "        self.reviews = positive + negative_shrunk\n",
    "        random.shuffle(self.reviews)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Split train dataset into train, validation and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# step 2.1: Load Data\n",
    "reviews = []\n",
    "data  = pd.read_excel('Data/finalDataset.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(data.astype(str) , columns = ['Sentence','SUBJprop'])\n",
    "# iterate elements of attribute \"Sentence\" and \"SUBJprop\" and push to the array \"reviews\"\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sentence = row['Sentence']\n",
    "    prop = row['SUBJprop']\n",
    "    reviews.append(Review(sentence,prop))\n",
    "\n",
    "print(\"Total Rows:\")\n",
    "print(len(reviews))\n",
    "print(\"Total Positive:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))))\n",
    "print(\"Total Negative:\")\n",
    "print(len(list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Rows:\n",
      "14058\n",
      "Total Positive:\n",
      "3904\n",
      "Total Negative:\n",
      "10154\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "neg_prop = list(filter(lambda x: x.propaganda == str(Propaganda.NEGATIVE), reviews))\n",
    "pos_prop = list(filter(lambda x: x.propaganda == str(Propaganda.POSITIVE), reviews))\n",
    "########################################################################################\n",
    "#split trainig and DevTest dataset\n",
    "neg_train, neg_devtest  = train_test_split(neg_prop , train_size=0.7, shuffle= False )\n",
    "pos_train, pos_devtest = train_test_split(pos_prop , train_size=0.7, shuffle= False )\n",
    "########################################################################################\n",
    "#prepare training dataset\n",
    "train = neg_train + pos_train\n",
    "#random.shuffle(train)\n",
    "########################################################################################\n",
    "#prepare development and test dataset\n",
    "neg_dev, neg_test = train_test_split(neg_devtest , train_size=0.5, shuffle= False )\n",
    "pos_dev, pos_test = train_test_split(pos_devtest , train_size=0.5, shuffle= False )\n",
    "\n",
    "dev = neg_dev + pos_dev\n",
    "#random.shuffle(dev)\n",
    "\n",
    "test = neg_test + pos_test\n",
    "#random.shuffle(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# step 3: Seperate the attribute, originally our array has text and score. we want them to be a seperate array\n",
    "train_container = ReviewContainer(train)\n",
    "train_container.evenly_distribute()\n",
    "\n",
    "train_text = train_container.get_sentence()   \n",
    "train_labels = train_container.get_propaganda() \n",
    "\n",
    "dev_text = [x.sentence for x in dev]\n",
    "dev_labels = [int(x.propaganda) for x in dev]\n",
    "\n",
    "test_text = [x.sentence for x in test]\n",
    "test_labels = [int(x.propaganda) for x in test]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print('Total Train Records:')\n",
    "print(len(train_text))\n",
    "print('Negative Train Records:')\n",
    "print(train_labels.count(0))\n",
    "print('Positive Train Records:')\n",
    "print(train_labels.count(1))\n",
    "print('\\n')\n",
    "print('Total Dev Records:')\n",
    "print(len(dev_text))\n",
    "print('Negative Dev Records:')\n",
    "print(dev_labels.count(0))\n",
    "print('Positive Dev Records:')\n",
    "print(dev_labels.count(1))\n",
    "print('\\n')\n",
    "print('Total Test Records:')\n",
    "print(len(test_text))\n",
    "print('Negative Test Records:')\n",
    "print(test_labels.count(0))\n",
    "print('Positive Test Records:')\n",
    "print(test_labels.count(1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Train Records:\n",
      "5464\n",
      "Negative Train Records:\n",
      "2732\n",
      "Positive Train Records:\n",
      "2732\n",
      "\n",
      "\n",
      "Total Dev Records:\n",
      "2109\n",
      "Negative Dev Records:\n",
      "1523\n",
      "Positive Dev Records:\n",
      "586\n",
      "\n",
      "\n",
      "Total Test Records:\n",
      "2110\n",
      "Negative Test Records:\n",
      "1524\n",
      "Positive Test Records:\n",
      "586\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save splited Data to seprate excel files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create Excel file of train dataset\n",
    "train_df = pd.DataFrame(train_text, columns=[\"Sentence\"])\n",
    "train_df['SUBJprop'] = train_labels\n",
    "train_df.to_excel(\"./Data/trainDataset.xlsx\")\n",
    "# Create Excel file of dev dataset\n",
    "dev_df = pd.DataFrame(dev_text, columns=[\"Sentence\"])\n",
    "dev_df['SUBJprop'] = dev_labels\n",
    "dev_df['Tanbih'] = \"\"\n",
    "dev_df.to_excel(\"./Data/devDataset.xlsx\")\n",
    "# Create Excel file of test dataset\n",
    "test_df = pd.DataFrame(test_text, columns=[\"Sentence\"])\n",
    "test_df['SUBJprop'] = test_labels\n",
    "test_df.to_excel(\"./Data/testDataset.xlsx\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization and Filtering Punctuation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizerNLTK = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train_text_tokenized = []\n",
    "dev_text_tokenized = []\n",
    "test_text_tokenized = []\n",
    "\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "    train_text_tokenized.append(tokenizerNLTK.tokenize(train_text[i])) \n",
    "\n",
    "for i in range(len(dev_text)):\n",
    "    dev_text_tokenized.append(tokenizerNLTK.tokenize(dev_text[i])) \n",
    "\n",
    "for i in range(len(test_text)):\n",
    "    test_text_tokenized.append(tokenizerNLTK.tokenize(test_text[i]))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(\"Before Sentence Tokenization:\")\n",
    "print(train_text[0:3])\n",
    "print(\"After Sentence Tokenization:\")\n",
    "print(train_text_tokenized[0:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Sentence Tokenization:\n",
      "['One way or another, the Church will have to repel an attacker at her very summit.\\n', 'As a firearms expert, he also questioned the lack of flashes coming from the hotel windows where the shooter was supposedly firing from.\\n', 'The investigation has been conducted by local and state police, not the feds, and yet, they continue to claim there is not Islamic terrorist involvement.\\n']\n",
      "After Sentence Tokenization:\n",
      "[['One', 'way', 'or', 'another', 'the', 'Church', 'will', 'have', 'to', 'repel', 'an', 'attacker', 'at', 'her', 'very', 'summit'], ['As', 'a', 'firearms', 'expert', 'he', 'also', 'questioned', 'the', 'lack', 'of', 'flashes', 'coming', 'from', 'the', 'hotel', 'windows', 'where', 'the', 'shooter', 'was', 'supposedly', 'firing', 'from'], ['The', 'investigation', 'has', 'been', 'conducted', 'by', 'local', 'and', 'state', 'police', 'not', 'the', 'feds', 'and', 'yet', 'they', 'continue', 'to', 'claim', 'there', 'is', 'not', 'Islamic', 'terrorist', 'involvement']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Repetition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train_text2=[]\n",
    "# SentimentWords\n",
    "\n",
    "# def checkIfDuplicates(listOfElems):\n",
    "#     if len(listOfElems) == len(set(listOfElems)):\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True\n",
    "\n",
    "# for sentence in train_text1:\n",
    "#     temp=[]\n",
    "#     if checkIfDuplicates(sentence):\n",
    "#         print(sentence)\n",
    "\n",
    "\n",
    "\n",
    "    # for word in sentence:\n",
    "    #     temp.append(word)\n",
    "\n",
    "    # train_text2.append(temp)\n",
    "\n",
    "# print(train_text2[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dictionaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# step 2.2: Load Sentimental Data\n",
    "SentimentWords= []\n",
    "SentimentValue= []\n",
    "\n",
    "\n",
    "Sentimentdata  = pd.read_excel('Data/DictionaryWords.xlsx', engine='openpyxl')\n",
    "df = pd.DataFrame(Sentimentdata.astype(str) , columns = ['word','value'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    word = row['word']\n",
    "    value = row['value']\n",
    "    SentimentWords.append(word)\n",
    "    SentimentValue.append(value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "\n",
    "# Create list of word tokens after removing stopwords in train_text\n",
    "train_text1=[]\n",
    "for sentence in train_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word) \n",
    "    train_text1.append(temp)\n",
    "\n",
    "# print(train_text1[:100])\n",
    "# print(\"*****************************************************\")\n",
    "\n",
    "# Create list of word tokens after removing stopwords in dev_text\n",
    "dev_text1 =[]\n",
    "\n",
    "for sentence in dev_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word) \n",
    "    dev_text1.append(temp)\n",
    "    \n",
    "# print(dev_text1[:100])\n",
    "# print(\"*****************************************************\")\n",
    "\n",
    "# # Create list of word tokens after removing stopwords in test_text\n",
    "test_text1 =[]\n",
    "\n",
    "for sentence in test_text_tokenized:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            temp.append(word)\n",
    "    test_text1.append(temp)\n",
    "# print(test_text1[:100])\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(train_labels[8])\n",
    "print('Before Removing Stopwords:')    \n",
    "print(train_text_tokenized[8])\n",
    "print('\\nAfter Removing Stopwords:') \n",
    "print(train_text1[8])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "Before Removing Stopwords:\n",
      "['Who', 'should', 'replace', 'Nikki', 'Haley', 'as', 'our', 'ambassador', 'to', 'the', 'U', 'N']\n",
      "\n",
      "After Removing Stopwords:\n",
      "['replace', 'Nikki', 'Haley', 'ambassador', 'U', 'N']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Duplicate Sentimenal words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train_text2=[]\n",
    "\n",
    "for sentence in train_text1:\n",
    "    temp=[]\n",
    "    for word in sentence:\n",
    "        temp.append(word)\n",
    "        for SentimentWord in SentimentWords:\n",
    "            if word.lower()==SentimentWord or word==SentimentWord:\n",
    "                temp.append(word)\n",
    "                temp.append(word)\n",
    "    train_text2.append(temp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# print(\"Propoganda:\")\n",
    "# print(train_labels[:64])\n",
    "print('Before Duplication:')\n",
    "print(train_text1[64])\n",
    "print('After Duplication:')\n",
    "print(train_text2[64])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before Duplication:\n",
      "['Muhammad', 'advises', 'thing', 'according', 'tradition', 'Muslim', 'doctors', 'wherefore', 'great', 'Temur', 'strove', 'exterminate', 'infidels', 'acquire', 'glory', 'signalise', 'greatness', 'conquests']\n",
      "After Duplication:\n",
      "['Muhammad', 'advises', 'thing', 'according', 'tradition', 'Muslim', 'Muslim', 'Muslim', 'doctors', 'wherefore', 'great', 'great', 'great', 'Temur', 'strove', 'exterminate', 'infidels', 'infidels', 'infidels', 'acquire', 'glory', 'signalise', 'greatness', 'conquests']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Detokenizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "train_text_detokenized=[]\n",
    "for i in range(len(train_text2)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(train_text2[i])):\n",
    "        temp=temp + \" \" + train_text2[i][j]        \n",
    "    train_text_detokenized.append(temp)\n",
    "# print(train_text_detokenized[:100])\n",
    "\n",
    "dev_text_detokenized=[]\n",
    "for i in range(len(dev_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(dev_text1[i])):\n",
    "        temp=temp + \" \" + dev_text1[i][j]        \n",
    "    dev_text_detokenized.append(temp)\n",
    "# print(dev_text_detokenized[:100])\n",
    "\n",
    "test_text_detokenized=[]\n",
    "for i in range(len(test_text1)):\n",
    "    temp=\"\"\n",
    "    for j in range(len(test_text1[i])):\n",
    "        temp=temp + \" \" + test_text1[i][j]        \n",
    "    test_text_detokenized.append(temp)\n",
    "# print(test_text_detokenized[:100])"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print(train_text_detokenized[60:64])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[' Committee Republicans considered political political political optics didn t relish prospect male team 11 senators questioning woman hired sex crimes prosecutor Rachel Mitchell Maricopa County Arizona Arizona Arizona ask questions promises highly watched hearing', ' Swiss bishop bishop bishop signs statement statement statement calling Pope s reading Amoris Laetitia alien Catholic Catholic Catholic Faith Faith Faith', ' called Francis Francis Francis resignation know degree certitude lawyer seeks Vigan√≤ s key allegations Francis Francis Francis substantially true assuredly reach conclusion true hesitation Francis Francis Francis resign', ' BAR SAFE SAFE SAFE SPACE']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "max_seq_len = 119\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text_detokenized,\n",
    "    # train_text2,\n",
    "    # is_split_into_words=True,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    dev_text_detokenized,\n",
    "    # dev_text1,\n",
    "    # is_split_into_words=True,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding='longest',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text_detokenized,\n",
    "    # test_text1,\n",
    "    # is_split_into_words=True,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    # padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Integer Sequences to Tensors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# for validation set\n",
    "dev_seq = torch.tensor(tokens_val['input_ids'])\n",
    "dev_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "dev_y = torch.tensor(dev_labels)\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "expected sequence of length 10 at dim 1 (got 15)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-09c9ec9b8198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 10 at dim 1 (got 15)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "print(train_seq[1])\n",
    "print(train_mask[1])\n",
    "print(train_y[1])\n",
    "print(train_labels[1])\n",
    "# print(test_seq)\n",
    "# print(test_mask)\n",
    "# print(test_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  101, 13780,  6739,  8781,  3768, 16121,  2746,  3309,  3645, 13108,\n",
      "        10743, 10743, 10743,  7493,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(0)\n",
      "0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel='rbf',C=1, probability=True, gamma=0.00000001)\n",
    "clf_svm.fit(train_seq, train_y)\n",
    "\n",
    "# clf_svm.predict(train_seq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=1e-08, probability=True)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tune using Training Data for SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "train_seq, test_seq, train_y, test_y = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), tuned_parameters, scoring='%s_macro' % score\n",
    "    )\n",
    "    clf.fit(train_seq, train_y)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = test_y, clf.predict(test_seq)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.959 (+/-0.028) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.026) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.983 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.988 (+/-0.017) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.983 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.974 (+/-0.012) for {'C': 1, 'kernel': 'linear'}\n",
      "0.974 (+/-0.012) for {'C': 10, 'kernel': 'linear'}\n",
      "0.974 (+/-0.012) for {'C': 100, 'kernel': 'linear'}\n",
      "0.974 (+/-0.012) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.97      1.00      0.98        90\n",
      "           2       0.99      0.98      0.98        92\n",
      "           3       1.00      0.99      0.99        93\n",
      "           4       1.00      1.00      1.00        76\n",
      "           5       0.99      0.98      0.99       108\n",
      "           6       0.99      1.00      0.99        89\n",
      "           7       0.99      1.00      0.99        78\n",
      "           8       1.00      0.98      0.99        92\n",
      "           9       0.99      0.99      0.99        92\n",
      "\n",
      "    accuracy                           0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.986 (+/-0.019) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.957 (+/-0.028) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.981 (+/-0.028) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.026) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.987 (+/-0.019) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.982 (+/-0.026) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.971 (+/-0.010) for {'C': 1, 'kernel': 'linear'}\n",
      "0.971 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
      "0.971 (+/-0.010) for {'C': 100, 'kernel': 'linear'}\n",
      "0.971 (+/-0.010) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        89\n",
      "           1       0.97      1.00      0.98        90\n",
      "           2       0.99      0.98      0.98        92\n",
      "           3       1.00      0.99      0.99        93\n",
      "           4       1.00      1.00      1.00        76\n",
      "           5       0.99      0.98      0.99       108\n",
      "           6       0.99      1.00      0.99        89\n",
      "           7       0.99      1.00      0.99        78\n",
      "           8       1.00      0.98      0.99        92\n",
      "           9       0.99      0.99      0.99        92\n",
      "\n",
      "    accuracy                           0.99       899\n",
      "   macro avg       0.99      0.99      0.99       899\n",
      "weighted avg       0.99      0.99      0.99       899\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "test_svm_predict = clf_svm.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2020])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_svm_predict[2000:2020])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:20])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_svm_predict[:20])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "X.shape[1] = 64 should be equal to 119, the number of features at training time",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-97fa946ab096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_svm_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual Lables:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Lables:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_svm_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2020\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    491\u001b[0m                                  (X.shape[1], self.shape_fit_[0]))\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_fit_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0m\u001b[1;32m    494\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                              (X.shape[1], self.shape_fit_[1]))\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 64 should be equal to 119, the number of features at training time"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_dec = DecisionTreeClassifier(random_state=0)\n",
    "clf_dec.fit(train_seq , train_y)\n",
    "\n",
    "# clf_dec.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dec_predict = clf_dec.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2050])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_dec_predict[2000:2050])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:50])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_dec_predict[:50])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_gnb = GaussianNB(var_smoothing=0.2848035868435802)\n",
    "clf_gnb.fit(train_seq , train_y)\n",
    "\n",
    "# clf_gnb.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tune using Training Data for Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "nbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\n",
    "nbModel_grid.fit(train_seq, train_y)\n",
    "print(nbModel_grid.best_estimator_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_gnb_predict = clf_gnb.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2050])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_gnb_predict[2000:2050])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:10])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_gnb_predict[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression()\n",
    "clf_log.fit(train_seq, train_y)\n",
    "\n",
    "# clf_log.predict(test_seq)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_log_predict = clf_log.predict(test_seq)\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[2000:2010])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_log_predict[2000:2010])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Actual Lables:\")\n",
    "print(test_labels[:10])\n",
    "print(\"Predicted Lables:\")\n",
    "print(test_log_predict[:10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## F1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(f1_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Decision Tree\n",
    "print(f1_score(test_y,clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Support Naive Bayes\n",
    "print(f1_score(test_y,clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "# For Logistic Regression\n",
    "print(f1_score(test_y,clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For Support Vector Machine\n",
    "print(clf_svm.score(test_seq,test_y))\n",
    "# For Decision Tree\n",
    "print(clf_dec.score(test_seq,test_y))\n",
    "# For Decision Naive Bayes\n",
    "print(clf_gnb.score(test_seq,test_y))\n",
    "# For Logistic Regression\n",
    "print(clf_log.score(test_seq,test_y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(precision_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(precision_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(precision_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(precision_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# For Support Vector Machine\n",
    "print(recall_score(test_y, clf_svm.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Tree\n",
    "print(recall_score(test_y, clf_dec.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Decision Naive Bayes\n",
    "print(recall_score(test_y, clf_gnb.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "# For Logistic Regression\n",
    "print(recall_score(test_y, clf_log.predict(test_seq),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VIP : Predict_Proba using Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdsvm = [] \n",
    "AboveThresholdsvmbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdsvm.append(test_text[i])\n",
    "    AboveThresholdsvmbrop.append(test_labels[i])\n",
    "\n",
    "print(\"Number of records above threshold SVM:\")\n",
    "print(len(AboveThresholdsvm))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Desicion Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdDT = [] \n",
    "AboveThresholdDTbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdDT.append(test_text[i])\n",
    "    AboveThresholdDTbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Desicion Tree:\")\n",
    "print(len(AboveThresholdDT))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdGNB = [] \n",
    "AboveThresholdGNBbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdGNB.append(test_text[i])\n",
    "    AboveThresholdGNBbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Logistic Regression:\")\n",
    "print(len(AboveThresholdGNB))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract Evaluation of Above Threshold Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "AboveThresholdLR = [] \n",
    "AboveThresholdLRbrop = []\n",
    "# [0][1] = Postivie\n",
    "# [0][0] = Negative\n",
    "\n",
    "\n",
    "for i in range(len(test_seq)):\n",
    "  if clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][0]>0.60:\n",
    "    AboveThresholdLR.append(test_text[i])\n",
    "    AboveThresholdLRbrop.append(test_labels[i])\n",
    "\n",
    "\n",
    "print(\"Number of records above threshold Logistic Regression:\")\n",
    "print(len(AboveThresholdLR))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(clf_log.predict_proba(test_seq[2002].reshape(1, -1))[0][1])\n",
    "print(clf_log.predict(test_seq[2002].reshape(1, -1)))\n",
    "print(test_labels[2002])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize Output above threshod"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bert\n",
    "tokens_testsvm = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdsvm,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqsvm = torch.tensor(tokens_testsvm['input_ids'])\n",
    "test_masksvm = torch.tensor(tokens_testsvm['attention_mask'])\n",
    "test_ysvm = torch.tensor(AboveThresholdsvmbrop)\n",
    "\n",
    "tokens_testDT = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdDT,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqDT = torch.tensor(tokens_testDT['input_ids'])\n",
    "test_maskDT = torch.tensor(tokens_testDT['attention_mask'])\n",
    "test_yDT = torch.tensor(AboveThresholdDTbrop)\n",
    "\n",
    "tokens_testGNB = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdGNB,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqGNB = torch.tensor(tokens_testGNB['input_ids'])\n",
    "test_maskGNB = torch.tensor(tokens_testGNB['attention_mask'])\n",
    "test_yGNB = torch.tensor(AboveThresholdGNBbrop)\n",
    "\n",
    "tokens_testLR = tokenizer.batch_encode_plus(\n",
    "    AboveThresholdLR,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "test_seqLR = torch.tensor(tokens_testLR['input_ids'])\n",
    "test_maskLR = torch.tensor(tokens_testLR['attention_mask'])\n",
    "test_yLR = torch.tensor(AboveThresholdLRbrop)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation Above Threshold"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#F1\n",
    "print(\"F1 SVM more than 60% threshold\")\n",
    "print(f1_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 DT more than 60% threshold\")\n",
    "print(f1_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 GNB more than 60% threshold\")\n",
    "print(f1_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"F1 LR more than 60% threshold\")\n",
    "print(f1_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Accuracy\n",
    "# For Support Vector Machine\n",
    "print(\"Accuracy SVM more than 60% threshold\")\n",
    "print(clf_svm.score(test_seqsvm,test_ysvm))\n",
    "\n",
    "print(\"Accuracy DT more than 60% threshold\")\n",
    "print(clf_dec.score(test_seqDT,test_yDT))\n",
    "\n",
    "print(\"Accuracy GNB more than 60% threshold\")\n",
    "print(clf_gnb.score(test_seqGNB,test_yGNB))\n",
    "\n",
    "print(\"Accuracy LR more than 60% threshold\")\n",
    "print(clf_log.score(test_seqLR,test_yLR))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "# For Support Vector Machine\n",
    "print(\"Precision SVM more than 60% threshold\")\n",
    "print(precision_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision DT more than 60% threshold\")\n",
    "print(precision_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision GNB more than 60% threshold\")\n",
    "print(precision_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Precision LR more than 60% threshold\")\n",
    "print(precision_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "print(\"Recall SVM more than 60% threshold\")\n",
    "print(recall_score(test_ysvm, clf_svm.predict(test_seqsvm),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall DT more than 60% threshold\")\n",
    "print(recall_score(test_yDT, clf_log.predict(test_seqDT),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall GNB more than 60% threshold\")\n",
    "print(recall_score(test_yGNB, clf_log.predict(test_seqGNB),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))\n",
    "\n",
    "print(\"Recall LR more than 60% threshold\")\n",
    "print(recall_score(test_yLR, clf_log.predict(test_seqLR),average = None, \n",
    "labels=[Propaganda.POSITIVE,Propaganda.NEGATIVE]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Output=[]\n",
    "for i in range(len(test_seq)):\n",
    "  Vote=0\n",
    "  if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  elif clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "    Vote=Vote+1\n",
    "  if (Vote>=1):\n",
    "    Output.append(\"Propaganda\")\n",
    "  else:\n",
    "    Output.append(\"nonPropaganda\")\n",
    "\n",
    "# for i in range(len(test_seq)):\n",
    "#   Vote=0\n",
    "#   if clf_svm.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_dec.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_gnb.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60 or clf_log.predict_proba(test_seq[i].reshape(1, -1))[0][1]>0.60:\n",
    "#     Vote=Vote+1\n",
    "#   if (Vote==1):\n",
    "#     Output.append(\"Propaganda\")\n",
    "#   else:\n",
    "#     Output.append(\"nonPropaganda\")\n",
    "\n",
    "# print(Output.count(Propaganda))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print Sample Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in range(2000,2100):\n",
    "    print(test[i].SUBJprop, end= '')\n",
    "\n",
    "print('\\n')\n",
    "for i in range(2000,2100):\n",
    "    x = clf_svm.predict(test_seq[i].reshape(1, -1))\n",
    "    print(x, end='')\n",
    "\n",
    "# print(len(test))\n",
    "# print(len(test_seq))\n",
    "\n",
    "# x = clf_svm.predict(test_seq[2].reshape(1, -1))\n",
    "# print(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "CountPropaganda=0\n",
    "for i in range(len(Output)):\n",
    "    if (Output[i]==\"nonPropaganda\"):\n",
    "        CountPropaganda=CountPropaganda+1\n",
    "\n",
    "print(\"number of propandas in output:\")\n",
    "print(CountPropaganda)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}